---
title: "MA40198 Coursework"
author: "Group 13: Sean Soon and Shaveen Colambage"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
load("C:\\Users\\soon\\Documents\\GitHub\\applied-statistical-inference\\.RData")
```

# Libraries 

```{r libraries, messages=FALSE, warning=FALSE}
library(rlang)
library(latex2exp)
```



# Part I

```{r dataQ1}
load(url("https://people.bath.ac.uk/kai21/ASI/data/CW24/dataQ1.RData"))
```

## Question 1.1


```{r Q1.1}
P2_optim_1 <- P2_optim_2 <- P2_optim_3 <- P2_optim_4 <- vector("list", 5)
names(P2_optim_1) <- names(P2_optim_2) <- names(P2_optim_3) <- 
  names(P2_optim_4) <- c("mle","negloglik","gradient","hessian","AIC")
P2_optim_1
```

The following functions `deriv_pack` and `fit_optim` will be used throughout 
the coursework. `fit_optim` has been modified from the solutions for Lab Sheet
5 to accept new arguments for compatibility with `deriv_pack` and the data sets
in this coursework.

```{r deriv_pack}
deriv_pack = function(func_expr, 
                      namevec = c("alpha", "beta", "theta"),
                      data_arg_vec = c("x", "y"),
                      theta_init = rep(1, 3)) {
  # Returns functions that separately evaluate the attributes of deriv
  # Inputs: 
  # - func_expr: Expr passed to deriv function
  # - namevec: Non-empty str vector that contains the names of the unknown
  #            parameters, passed to deriv function
  # - data_arg_vec: Non-empty string vector that contains the names of the data 
  #                 variables, passed to deriv function
  # - theta_init: Numerical vector of equal length to namevec that represents
  #               the default argument of the functions in the output.
  # Output: List of length 2 containing two functions:
  # - fn: Function that evaluates func_expr
  # - gr: Function that evaluates the gradient of func_expr
  
  deriv_res <- deriv(
    func_expr,
    namevec = namevec,
    function.arg = c(namevec, data_arg_vec)
  )
  
  fn <- function(theta = theta_init, data_list = list("x" = 1, "y" = 1), 
                 apply.sum = T) {
    # Evaluates func_expr.
    # Input:
    # - theta: Numerical vector representing the parameters.
    # - data_list: Data.frame or list representing the data set.
    # - apply.sum: Logical representing whether the results from evaluating
    #              func_expr on each data point in data_list are summed.
    # Output: if apply.sum = T, then a float is returned; otherwise, a matrix
    # is returned.
    theta_list = as.list(theta)
    names(theta_list) = namevec
    aux  <- do.call(deriv_res, c(theta_list, data_list))
    if(apply.sum) {
      sum(as.numeric(aux))
    } else {
      c(aux)
    } 
  }
  
  gr <- function(theta = theta_init, data_list = list("x" = 1, "y" = 1), 
                 apply.sum = T) {
    # Evaluates the gradient of func_expr.
    # Input:
    # - theta: Numerical vector representing the parameters.
    # - data_list: Data.frame or list representing the data set.
    # - apply.sum: Logical representing whether the results from evaluating
    #              func_expr on each data point in data_list are summed.
    # Output: if apply.sum = T, then a float is returned; otherwise, a matrix
    # is returned.
    theta_list = as.list(theta)
    names(theta_list) = namevec
    aux  <- do.call(deriv_res, c(theta_list, data_list))
    if(apply.sum) {
      apply(attr(aux, "gradient"), 2, sum)
    } else {
      attr(aux, "gradient")
    }
  }
  
  list(fn = fn, gr = gr)
}
``` 

```{r fit_optim}
fit_optim <- function(par_len, nll_pack, data, mean = 0, sd = 10, 
                      method = "BFGS", hessian = T, N_samples = 100, 
                      silent = T){
  # Runs N_samples optimisations and returns the best result.
  # Inputs: 
  # - par_len: Int that represents the length of the unknown parameter vector
  # - nll_pack: List that contains two functions:
  #    - fn: Function to be minimised.
  #    - gr: Function that evaluates the gradient of fn.
  # - data: Data.frame or list that represents the data set.
  # - mean: Float that represents the mean of the normal distribution the 
  #         initial points for the optimisations are generated from.
  # - sd: Float that represents the sd of the normal distribution the 
  #       initial points for the optimisations are generated from.
  # - method: Str that represents the optimisation method to be used.
  # - hessian: Logical that represents whether the hessian is estimated for all
  #            the optimisations
  # - N_samples: Int that represents the number of optimisations to run.
  # - silent: Logical that represents whether error messages are suppressed.
  # Outputs: the same output as optim
  
  fit <- vector("list", length = N_samples)
  for (i in 1:N_samples){
    
    stop_crit <-T
    
    # Attempt an optimisation until there is no upfront error. 
    # This guarantees that N_samples optimisations are done.
    while(stop_crit){
      
      # try is used as the initial point may occassionally be too far, which 
      # throws a straight error.
      fit[[i]] <- try( 
        optim(par = rnorm(par_len, mean = mean, sd = sd),
              fn  = nll_pack$fn,
              gr  = nll_pack$gr,
              data_list = data,
              method  = method ,
              hessian = hessian),
        silent = silent)
      
      if(inherits(fit[[i]], "try-error")){
        
        stop_crit <-T # if there is an error, try again
        
      } else {
        
        stop_crit <-F # if there is no error, continue to the next step.
      }
    }
    
    # Check if there is no numerical convergence.
    no_convergence <- fit[[i]]$convergence > 0
    
    # Check if asymptotic variances are obtainable.
    no_variance <- inherits(try(solve(fit[[i]]$hessian),
                                silent = T), 
                            "try-error")
    
    null_variance <- F
    NA_variance   <- F
    
    if (!no_variance){
      #  Check if any of the asymptotic variances are NaN.
      NA_variance <- as.logical(sum(is.nan(diag(solve(fit[[i]]$hessian)))))
      
      if(!NA_variance){
        # Check if any of the asymptotic variances are zero up to machine precision
        null_variance <- as.logical(sum(diag(solve(fit[[i]]$hessian)) < 
                                          .Machine$double.eps ^ 0.5))
      }
    }
    
    fail <- no_variance | no_convergence | NA_variance | null_variance 
    if (fail){
      fit[[i]]$value <- NA
    }
    
  } 
  
  extract_negloglik <- function(optim_object){
    optim_object$value
  }
  # Select the optimisation with minimum negative log-likelihood.
  nll_vals <- lapply(X = fit, FUN = extract_negloglik)
  
  # Return the selected optimisation.
  fit[[which.min(nll_vals)]] 
}
```

```{r set_P2_optim}
set_P2_optim = function(P2_optim, nll_pack, opt_res) {
  # Calculates and stores the required values in the given P2_optim list.
  # Inputs:
  # - P2_optim: P2_optim_1, P2_optim_2, P2_optim_3, or P2_optim_4 list.
  # - nll_pack: deriv_pack output.
  # - opt_res: optim output.
  # Output: P2_optim with the required values.
  
  # Store MLE, NLL, and hessian from opt_res.
  P2_optim$mle = opt_res$par
  P2_optim$negloglik = opt_res$value
  P2_optim$hessian = opt_res$hessian
  
  # Calculate the gradient at the MLE.
  P2_optim$gradient = nll_pack$gr(opt_res$par, dataQ1)
  
  # Calculate the AIC.
  P2_optim$AIC = 2 * (opt_res$value + length(opt_res$par))
  
  # Return P2_optim
  P2_optim
}
```

$\sigma$ is reparametrised as $\exp(\theta)$ in all the models. 

```{r P2_optim_1, warning=FALSE}
# Define the NLL expression for model 1.
nll_expr_q1_1 = expr(0.5 * log(2 * pi) + theta + alpha + beta * x + 
                       0.5 * (y - exp(alpha + beta * x)) ^ 2 / 
                       exp(2 * (theta + alpha + beta * x)))

# Run the optimisations for model 1.
nll_pack_q1_1 = deriv_pack(nll_expr_q1_1)
opt_res_q1_1 = fit_optim(3, nll_pack_q1_1, dataQ1)

# Store the results in P2_optim_1.
set_P2_optim(P2_optim_1, nll_pack_q1_1, opt_res_q1_1)
```

```{r P2_optim_2, warning=FALSE}
# Define the NLL expression for model 2.
nll_expr_q1_2 = expr(0.5 * log(2 * pi) + theta + 
                       0.5 * (log(y) - alpha - beta * x) ^ 2 / exp(2 * theta))
nll_pack_q1_2 = deriv_pack(nll_expr_q1_2)

# Run the optimisations for model 2.
opt_res_q1_2 = fit_optim(3, nll_pack_q1_2, dataQ1)

# Store the results in P2_optim_2.
set_P2_optim(P2_optim_2, nll_pack_q1_2, opt_res_q1_2)
```

```{r P2_optim_3, warning=FALSE}
# Define the NLL expression for model 3.
z_expr_q1_3 = expr((log(y) - alpha - beta * x) / exp(theta))
nll_expr_q1_3 = expr(!!z_expr_q1_3 + 2 * log((1 + exp(-!!z_expr_q1_3))) + theta)
nll_pack_q1_3 = deriv_pack(nll_expr_q1_3)

# Run the optimisations for model 3.
opt_res_q1_3 = fit_optim(3, nll_pack_q1_3, dataQ1)

# Store the results in P2_optim_3.
set_P2_optim(P2_optim_3, nll_pack_q1_3, opt_res_q1_3)
```

```{r P2_optim_4, warning=FALSE}
# Define the NLL expression for model 4.
nll_expr_q1_4 = expr(
  -log(log(2)) - theta + exp(theta) * (alpha + beta * x) + (1 - exp(theta)) * 
    log(y) + log(2) * (y / exp(alpha + beta * x)) ^ exp(theta)
)
nll_pack_q1_4 = deriv_pack(nll_expr_q1_4)

# Run the optimisations for model 4.
opt_res_q1_4 = fit_optim(3, nll_pack_q1_4, dataQ1)

# Store the results in P2_optim_4.
set_P2_optim(P2_optim_4, nll_pack_q1_4, opt_res_q1_4)
```

Model 3 has the lowest AIC and so, is the best model based on the AIC.

## Question 1.2

```{r Q1.2}
n_grid     <- 1000 
x          <- seq(1, 15, length=n_grid)
P2_bands_1 <- P2_bands_2 <- 
  P2_bands_3 <- P2_bands_4 <- matrix(NA,nrow=n_grid,ncol=3)
colnames(P2_bands_1) <- colnames(P2_bands_2) <- 
  colnames(P2_bands_3) <- colnames(P2_bands_4) <- c("lower","est","upper")
head(P2_bands_1)
```

The mean of $Y|x$ based on models 2 and 3 are $\exp(\alpha_{2} + \beta_{2} x)$
and $\exp(\alpha_{3} + \beta_{3} x)$, respectively. Thus, $Y|x$ has median 
$\mu(\alpha, \beta, x) = \exp(\alpha + \beta x)$ under all the models. The 95%
confidence interval (CI) of the median can be calculated by calculating the 95%
CI for $\alpha + \beta x$ then taking the exponential of the bounds as $\exp$
is monotone ([Evangelou 2024](#references)). 

```{r set P2_bands}
set_P2_bands = function(P2_bands, opt_res) {
  # Calculates the mean and its 95% CI for Q1.2.
  # Inputs:
  # - P2_bands: P2_bands_1, P2_bands_2, P2_bands_3, or P2_bands_4 matrix.
  # - opt_res: optim output
  # Output: P2_bands
  temp = P2_bands
  S <- solve(opt_res$hessian) 
  for (i in 1:n_grid){
    vec.x <- c(1, x[i])
    est <- crossprod(vec.x, opt_res$par[1:2])
    se <- sqrt(crossprod(vec.x, S[1:2, 1:2]) %*% vec.x)
    temp[i,] <- exp(c(est - 1.96 * se, est, est + 1.96 * se))
  }
  temp
}
```

The following function `plot_bands` will be used throughout the coursework.

```{r plot_bands}
plot_bands = function(bands, data, main = "") {
  # Plots the data, mean function and its CI.
  # Inputs:
  # - bands: Matrix with the first three columns representing the lower bound,
  #          of the CI, the estimated mean, and the upper bound of the CI.
  # - data: Data.frame or list representing the data set.
  # - main: Str representing the plot title.
  plot(data, pch = 19, main = main)
  lines(x, bands[, 2], col = "red")
  lines(x, bands[, 1], col = "red", lty = 2)
  lines(x, bands[, 3], col = "red", lty = 2)
}
```

```{r P2_bands_1}
# Calculate and plot the median and its 95% CI for model 1.
P2_bands_1 = set_P2_bands(P2_bands_1, opt_res_q1_1)
plot_bands(P2_bands_1, dataQ1, 
           main = "Plot of the median and its 95% CI for model 1")
```

```{r P2_bands_2}
# Calculate and plot the median and its 95% CI for model 2.
P2_bands_2 = set_P2_bands(P2_bands_2, opt_res_q1_2)
plot_bands(P2_bands_2, dataQ1, 
           main = "Plot of the median and its 95% CI for model 2")
```

```{r P2_bands_3}
# Calculate and plot the median and its 95% CI for model 3.
P2_bands_3 = set_P2_bands(P2_bands_3, opt_res_q1_3)
plot_bands(P2_bands_3, dataQ1,
           main = "Plot of the median and its 95% CI for model 3")
```

```{r P2_bands_4}
# Calculate and plot the median and its 95% CI for model 4.
P2_bands_4 = set_P2_bands(P2_bands_4, opt_res_q1_4)
plot_bands(P2_bands_4, dataQ1,
           main = "Plot of the median and its 95% CI for model 4")
```

# Part II

```{r dataQ2}
load(url("https://people.bath.ac.uk/kai21/ASI/data/CW24/dataQ2.RData"))
```

## Question 2.1

Under the four models, $Y$ follows a Gamma distribution with shape 
parameter $k$ and rate parameter $\lambda$. Thus,

$$
E(Y) = \frac{k}{\lambda}, \quad 
\mathrm{Var}(Y) = \frac{k}{\lambda^{2}}
$$

Substituting $E(Y) = \mu$ and $\mathrm{Var}(Y) = \phi \mu^{2}$ with 
$\mu(\alpha, \beta, x) = \frac{1}{\alpha + \beta x}$ under the full model into 
the equations above yields

$$
k = \frac{1}{\phi}, \quad
\lambda = \frac{1}{\phi \mu} = \frac{\alpha + \beta x}{\phi}
$$

Under the different sub-models, the respective values of $\alpha$, $\beta$, and 
$\phi$ can be substituted accordingly.

```{r Q2.1}
P21_optim_1 <- P21_optim_2 <- P21_optim_3 <- P21_optim_full <- P25_optim <- 
  vector("list", 5)
names(P21_optim_1) <- names(P21_optim_2) <- names(P21_optim_3) <- 
  names(P21_optim_full) <- names(P25_optim) <-
  c("mle","negloglik","gradient","hessian","NIC")
P21_optim_1
```

```{r nic}
nic = function(nll_pack, opt_res, data = dataQ2) {
  # Computes the NIC.
  # Input: 
  # - nll_pack: List containing two functions:
  #    - fn: Function that evaluates the model.
  #    - gr: Function that returns the gradient of fn.
  # - opt_res: optim output
  # - data: Data.frame or list representing the data set.
  # Output: Float representing the NIC.
  
  aux = nll_pack$gr(opt_res$par, data, apply.sum = F)
  2 * (opt_res$value + sum(diag(solve(opt_res$hessian) %*% crossprod(aux))))
}
```

For $\mathcal{F}_{\text{full}}$, $\phi$ is reparametrised as $\exp(\theta)$.

```{r P21_optim_full, warning=FALSE}
# Define the NLL expression for the full model.
nll_expr_q2_full = expr(lgamma(exp(-theta)) - 
                          exp(-theta) * (log(alpha + beta * x) - theta) + 
                          (1 - exp(-theta)) * log(y) + 
                          exp(-theta) * (alpha + beta * x) * y)
nll_pack_q2_full = deriv_pack(nll_expr_q2_full)

# Run the optimisations.
opt_res_q2_full = fit_optim(3, nll_pack_q2_full, dataQ2)

# Store the results in P21_optim_full.
P21_optim_full$mle = opt_res_q2_full$par
P21_optim_full$negloglik = opt_res_q2_full$value
P21_optim_full$hessian = opt_res_q2_full$hessian
P21_optim_full$gradient = nll_pack_q2_full$gr(opt_res_q2_full$par, dataQ2)
P21_optim_full$NIC = nic(nll_pack_q2_full, opt_res_q2_full)
P21_optim_full
```

For $\mathcal{F}_{1}$, $\mu(\theta_{1}, -\theta_{1}^{2}, x) > 0 \iff \theta_{1} 
> \theta_{1} ^{2}$ as $x \in (0, 1]$. Thus, $\theta_{1}$ is reparametrised 
using the logit function, i.e. set $\log \frac{\theta_{1}}{1 - \theta_{1}} = p$. Equivalently,  
$\theta_{1} = \frac{1}{1 + \exp(p)}$.

```{r P21_optim_1, warning=FALSE}
# Define the NLL expression for model 1. 
theta1_expr = expr(1/(1 + exp(p)))
nll_expr_q2_1 = expr(
  lgamma(exp(-!!theta1_expr)) - exp(-!!theta1_expr) * 
    (log(!!theta1_expr - x * (!!theta1_expr) ^ 2) - !!theta1_expr) +
    (1 - exp(-!!theta1_expr)) * log(y) + exp(-!!theta1_expr) * 
    (!!theta1_expr - x * (!!theta1_expr) ^ 2) * y)
nll_pack_q2_1 = deriv_pack(nll_expr_q2_1,
                           c("p"),
                           theta_init = 1)

# Run the optimisations.
opt_res_q2_1 = fit_optim(1, nll_pack_q2_1, dataQ2)

# Store the results in P21_optim_1.
P21_optim_1$mle = opt_res_q2_1$par
P21_optim_1$negloglik = opt_res_q2_1$value
P21_optim_1$hessian = opt_res_q2_1$hessian
P21_optim_1$gradient = nll_pack_q2_1$gr(opt_res_q2_1$par, dataQ2)
P21_optim_1$NIC = nic(nll_pack_q2_1, opt_res_q2_1)
P21_optim_1
```

No reparametrisations are needed for $\mathcal{F}_{2}$.

```{r P21_optim_2, warning=FALSE}
# Define the NLL expression for model 2.
nll_expr_q2_2 = expr(lgamma(exp(-theta2)) - 
                       exp(-theta2) * (log(theta2 + tau2 * x) - theta2) + 
                       (1 - exp(-theta2)) * log(y) + 
                       exp(-theta2) * (theta2 + tau2 * x) * y)
nll_pack_q2_2 = deriv_pack(nll_expr_q2_2,
                           c("theta2", "tau2"),
                           theta_init = rep(1, 2))

# Run the optimisations.
opt_res_q2_2 = fit_optim(2, nll_pack_q2_2, dataQ2)

# Store the results in P21_optim_2.
P21_optim_2$mle = opt_res_q2_2$par
P21_optim_2$negloglik = opt_res_q2_2$value
P21_optim_2$hessian = opt_res_q2_2$hessian
P21_optim_2$gradient = nll_pack_q2_2$gr(opt_res_q2_2$par, dataQ2)
P21_optim_2$NIC = nic(nll_pack_q2_2, opt_res_q2_2)
P21_optim_2
```

For $\mathcal{F}_{3}$, $\theta_{3}$ is reparametrised as
\frac{1}{1 + \exp(p)} similar to $\mathcal{F}_{1}$. Besides that, $\delta_{3}$ 
is reparametrised as $\exp(\vartheta)$.

```{r P21_optim_3, warning=FALSE}
# Define the NLL expression for model 3.
theta3_expr = expr(1/(1 + exp(p)))
nll_expr_q2_3 = expr(
  lgamma(exp(-vtheta)) - exp(-vtheta) * 
    (log(!!theta3_expr - x * (!!theta3_expr) ^ 2) - vtheta) + 
    (1 - exp(-vtheta)) * log(y) + exp(-vtheta) * 
    (!!theta3_expr - x * (!!theta3_expr) ^ 2) * y
)
nll_pack_q2_3 = deriv_pack(nll_expr_q2_3,
                           c("p", "vtheta"),
                           theta_init = rep(2, 2))

# Run the optimisations.
opt_res_q2_3 = fit_optim(2, nll_pack_q2_3, dataQ2)

# Store the results in P21_optim_3.
P21_optim_3$mle = opt_res_q2_3$par
P21_optim_3$negloglik = opt_res_q2_3$value
P21_optim_3$hessian = opt_res_q2_3$hessian
P21_optim_3$gradient = nll_pack_q2_3$gr(opt_res_q2_3$par, dataQ2)
P21_optim_3$NIC = nic(nll_pack_q2_3, opt_res_q2_3)
P21_optim_3
```

The full model has the lowest NIC and so, is the best model based on the NIC.

## Question 2.2 

```{r Q2.2}
n_grid     <- 1000 
x          <- seq(0, 1, length=n_grid)
P22_bands_1 <- P22_bands_2 <- 
  P22_bands_3 <- P22_bands_full <- matrix(NA,nrow=n_grid,ncol=3)
colnames(P22_bands_1) <- colnames(P22_bands_2) <- 
  colnames(P22_bands_3) <- colnames(P22_bands_full) <- c("lower","est","upper")
head(P22_bands_1)
```

Since the reciprocal function is monotone, the 95% CI of the mean under
$\mathcal{F}_{\text{full}}$ and $\mathcal{F}_{2}$ can be calculated similarly 
as in [Q1.2](#q1.2). The only difference in the approach is that bounds need 
to be swapped as $f$ is decreasing ([Evangelou 2024](#references)).

```{r P22_bands}
S_full <- solve(opt_res_q2_full$hessian)  
S_2 <- solve(opt_res_q2_2$hessian)

for (i in 1:n_grid){
  # Calculate the mean and its 95% CI under the full model.
  vec.x <- c(1, x[i])
  est_full <- crossprod(vec.x, opt_res_q2_full$par[1:2])
  se_full <- sqrt(crossprod(vec.x, S_full[1:2, 1:2]) %*% vec.x)
  P22_bands_full[i,] <- 1/c(est_full + 1.96 * se_full, 
                         est_full, 
                         est_full - 1.96 * se_full) 
  
  # Calculate the mean and its 95% CI under model 2.
  est_2 <- crossprod(vec.x, opt_res_q2_2$par[1:2])
  se_2 <- sqrt(crossprod(vec.x, S_2[1:2, 1:2]) %*% vec.x)
  P22_bands_2[i,] <- 1/c(est_2 + 1.96 * se_2, 
                         est_2, 
                         est_2 - 1.96 * se_2)
}
```

For models 1 and 3, the 95% CI can be calculated using the delta method as per
Proposition 3.4.

```{r mean_delta_method}
mean_delta_method = function(bands, mean_expr, namevec, par, cov_mat) {
  # Calculates the mean and its 95% CI using the delta method.
  # Inputs:
  # - bands: Matrix with the first three columns representing the lower bound,
  #          of the CI, the estimated mean, and the upper bound of the CI.
  # - mean_expr: Expr of the mean function.
  # - namevec: Non-empty str vector containing the names of the estimated
  #            parameters in the mean function.
  # - par: Numerical vector containing the estimated parameter values.
  # - cov_mat: Covariance matrix for the estimated parameters.
  # Output: bands matrix
  
  temp = bands
  mean_pack = deriv_pack(mean_expr, namevec, c("x"))
  for (i in 1:n_grid) {
    est = mean_pack$fn(par, list("x" = x[i]))
    J = mean_pack$gr(par, list("x" = x[i]))
    se = sqrt(J %*% cov_mat %*% J)
    temp[i,] = est + c(- 1.96 * se, 0, 1.96 * se)
  }
  temp
}
```

```{r set_P22_bands}
set_P22_bands = function(P22_bands, mean_expr, namevec, opt_res) {
  # Calculates the mean and its 95% CI using the delta method with the hessian  
  # of the NLL as the covariance matrix.
  # Inputs:
  # - bands: P22_bands_full, P22_bands_1, P22_bands_2, P22_bands_3 matrix. 
  # - mean_expr: Expr of the mean function.
  # - namevec: Non-empty str vector containing the names of the estimated
  #            parameters in the mean function.
  # - opt_res: optim output.
  # Output: bands matrix
  
  mean_delta_method(P22_bands, mean_expr, namevec, opt_res$par, 
                    solve(opt_res$hessian))
}
```

```{r P22_bands_full}
# Plot the mean and its 95% CI for the full model.
plot_bands(P22_bands_full, dataQ2,
           main = "Plot of the mean and its 95% CI for the full model")
```

```{r P22_bands_1}
# Calculate and plot the mean and its 95% CI for model 1.
P22_bands_1 = set_P22_bands(P22_bands_1, 
                           expr((1 + exp(p)) ^ 2/(1 + exp(p) - x)), 
                           c("p"), 
                           opt_res_q2_1)
plot_bands(P22_bands_1, dataQ2, 
           main = "Plot of the mean and its 95% CI for model 1")
```

```{r P22_bands_2}
# Plot the mean and its 95% CI for model 2.
plot_bands(P22_bands_2, dataQ2,
           main = "Plot of the mean and its 95% CI for model 2")
```

```{r P22_bands_3} 
# Calculate and plot the mean and its 95% CI for model 3.
P22_bands_3 = set_P22_bands(P22_bands_3, 
                           expr((1 + exp(p)) ^ 2/(1 + exp(p) - x)), 
                           c("p", "vtheta"), 
                           opt_res_q2_3)
plot_bands(P22_bands_3, dataQ2, 
           main = "Plot of the mean and its 95% CI for model 3")
```

## Question 2.3

The 95% CI of the mean for a misspecified model can be estimated as per 
Proposition 3.7. For an unknown $\boldsymbol{\theta}^{\dagger}$,
$\boldsymbol{\mathcal{J}}(\boldsymbol{\theta}^{\dagger})$ and
$\boldsymbol{\mathcal{K}}(\boldsymbol{\theta}^{\dagger})$ can be estimated 
using equations 3.14 and 3.15, respectively.

```{r Q2.3}
n_grid <- 1000 
x <- seq(0,1,length=n_grid)
P23_bands_1 <- P23_bands_2 <- 
  P23_bands_3 <- P23_bands_full <- P25_bands <- matrix(NA,nrow=n_grid,ncol=3)
colnames(P23_bands_1) <- colnames(P23_bands_2) <- 
  colnames(P23_bands_3) <- colnames(P23_bands_full) <- colnames(P25_bands) <-
  c("lower","est","upper")
head(P23_bands_1)
```

```{r P23_bands}
P23_bands = function(bands, mean_expr, namevec, nll_pack, opt_res) {
  # Calculates the mean and its 95% CI under a misspecified model using the 
  # delta method.
  # Inputs:
  # - bands: P22_bands_full, P22_bands_1, P22_bands_2, P22_bands_3 matrix. 
  # - mean_expr: Expr of the mean function.
  # - namevec: Non-empty str vector containing the names of the estimated
  #            parameters in the mean function.
  # - nll_pack: deriv_pack output.
  # - opt_res: optim output.
  # Output: bands matrix
  
  # Calculate the estimated covariance matrix.
  S = solve(opt_res$hessian)
  nll_gr = nll_pack$gr(opt_res$par, dataQ2, apply.sum = F)
  Khat = crossprod(nll_gr)
  cov_mat = S %*% Khat %*% S
  
  # Return the mean and its 95% CI.
  mean_delta_method(bands, mean_expr, namevec, opt_res$par, cov_mat)
}
```

```{r P23_bands_alt}
# Calculate the covariance matrix of the asymptotic distribution of the MLE 
# for the misspecified full model.
S_full = solve(opt_res_q2_full$hessian)
nll_gr_q2_full = nll_pack_q2_full$gr(opt_res_q2_full$par, dataQ2, apply.sum = F)
Khat_full = crossprod(nll_gr_q2_full)
cov_full = S_full %*% Khat_full %*% S_full

# Calculate the covariance matrix of the asymptotic distribution of the MLE 
# for the misspecified model 2.
S_2 = solve(opt_res_q2_2$hessian)
nll_gr_q2_2 = nll_pack_q2_2$gr(opt_res_q2_2$par, dataQ2, apply.sum = F)
Khat_2 = crossprod(nll_gr_q2_2)
cov_2 = S_2 %*% Khat_2 %*% S_2

for (i in 1:n_grid){
  # Calculate the mean and its 95% CI under the misspecified full model.
  vec.x <- c(1, x[i])
  est_full <- crossprod(vec.x, opt_res_q2_full$par[1:2])
  se_full <- sqrt(crossprod(vec.x, cov_full[1:2, 1:2]) %*% vec.x)
  P23_bands_full[i,] <- 1/c(est_full + 1.96 * se_full,
                         est_full,
                         est_full - 1.96 * se_full)
  
  # Calculate the mean and its 95% CI under the misspecified model 2.
  est_2 <- crossprod(vec.x, opt_res_q2_2$par[1:2])
  se_2 <- sqrt(crossprod(vec.x, cov_2[1:2, 1:2]) %*% vec.x)
  P23_bands_2[i,] <- 1/c(est_2 + 1.96 * se_2,
                         est_2,
                         est_2 - 1.96 * se_2)
}
```

```{r P23_bands_full}
# Plot the mean function and the 95% CI for the full model.
plot_bands(P23_bands_full, dataQ2,
           main = paste(c("Plot of the least-worse mean and its 95% CI for",
                          "the full model"))
           )
```

```{r P23_bands_1}
# Calculate and plot the mean function and the 95% CI for model 1.
P23_bands_1 = P23_bands(P23_bands_1, 
                        expr((1 + exp(p)) ^ 2/(1 + exp(p) - x)), 
                        c("p"), 
                        nll_pack_q2_1,
                        opt_res_q2_1)
plot_bands(P23_bands_1, dataQ2, 
           main = "Plot of the least-worse mean and its 95% CI for model 1")
```

```{r P23_bands_2}
# Plot the mean function and the 95% CI for model 2.
plot_bands(P23_bands_2, dataQ2,
           main = "Plot of the least-worse mean and its 95% CI for model 2")
```

```{r P23_bands_3} 
# Calculate and plot the mean function and the 95% CI for model 3.
P23_bands_3 = P23_bands(P23_bands_3, 
                        expr((1 + exp(p)) ^ 2/(1 + exp(p) - x)), 
                        c("p", "vtheta"), 
                        nll_pack_q2_3,
                        opt_res_q2_3)
plot_bands(P23_bands_3, dataQ2, 
           main = "Plot of the least-worse mean and its 95% CI for model 3")
```

## Question 2.4

With $\alpha^{*} = \frac{2}{3}$, $\beta^{*} = -\frac{1}{3}$, and 
$\phi^{*} = \frac{1}{2}$, the correct shape and rate parameters are 
$\frac{1}{\phi^{*}} = 2$ and 
$\frac{\alpha^{*} + \beta^{*}x}{\phi^{*}} = \frac{4 - 2x}{3}$, respectively.
By Definition 3.11, $\boldsymbol{\theta}^{\dagger}$ can be found numerically 
for each model by minimising the KL divergence.

```{r P24_optim, eval=FALSE}
P24_optim_1 <- P24_optim_2 <- P24_optim_3 <- vector("list", 3)
names(P24_optim_1) <- names(P24_optim_2) <- names(P24_optim_3) <- 
  c("mle","kl","gradient")
P24_optim_1
```

```{r P24_optim_blank, echo=FALSE}
P24_optim_blank <- vector("list", 3)
names(P24_optim_blank) <- c("mle","kl","gradient")
P24_optim_blank
```

```{r P24_bands}
n_grid     <- 1000 
x          <- seq(0, 1,length=n_grid)
P24_bands_1 <- P24_bands_2 <- 
  P24_bands_3 <- P24_bands_full <- matrix(NA,nrow=n_grid,ncol=3)
colnames(P24_bands_1) <- colnames(P24_bands_2) <- 
  colnames(P24_bands_3) <- colnames(P24_bands_full) <- c("lower","est","upper")
head(P24_bands_1)
```

```{r KL_pack}
KL_pack = function(nll_pack) {
  # Defines functions for computing the KL divergence and its gradient.
  # Input: 
  # - nll_pack: Output from deriv_pack
  # Output: List of two functions:
  # - fn: Function that evaluates the KL divergence.
  # - gr: Function that calculates the gradient of the KL divergence.
  
  fn_integrand = function(y, x, theta) {
    # Evaluates the expression that is integrated with respect to y to 
    # calculate the KL divergence.
    # Input:
    # - y: Numerical vector
    # - x: Float
    # - theta: Numerical vector representing the parameter vector
    # Output: Float
    
    (nll_pack$fn(theta, list("x" = x, "y" = y), apply.sum = F) +
      dgamma(y, 2, (4 - 2 * x)/3, log = T)) * 
      dgamma(y, 2, (4 - 2 * x)/3)
  }
  gr_integrand = function(y, x, theta, col_int = 1) {
    # Evaluates the expression that is integrated with respect to y 
    # to calculate the gradient of the KL divergence.
    # Input:
    # - y: Numerical vector
    # - x: Float
    # - theta: Numerical vector representing the parameter vector
    # - col_int: Int representing the parameter the gradient is 
    #            calculated with respect to.
    # Output: Float
    aux = nll_pack$gr(theta, list("x" = x, "y" = y), apply.sum = F) *
      dgamma(y, 2, (4 - 2 * x)/3)
    aux[, col_int]
  }
  KL_fn = function(theta, data_list = dataQ2) {
    # Evaluates the KL divergence.
    # Input: 
    # - theta: Numerical vector representing the parameter vector.
    # - data_list: Unused argument for compatibility with fit_optim.
    # Output: Float
    
    res = 1:10
    for(i in res) {
      res[i] = integrate(fn_integrand, 0, 10, x = i/10, 
                         theta = theta)$value
    }
    sum(res)
  }
  
  KL_gr = function(theta, data_list = dataQ2) {
    # Calculates the gradient of the KL divergence.
    # Input: 
    # - theta: Numerical vector representing the parameter vector.
    # - data_list: Unused argument for compatibility with fit_optim.
    # Output: Float
    
    n = length(theta)
    res = matrix(NA, nrow = 10, ncol = n)
    for(i in 1:10) {
      for(j in 1:n) {
        res[i, j] = integrate(gr_integrand, 0, 10, x = i/10, 
                              theta = theta, col_int = j)$value
      }
    }
    apply(res, 2, sum)
  }
  
  list(fn = KL_fn, gr = KL_gr)
}
```

```{r P24_optim_1, warning=FALSE, error=FALSE, eval=FALSE}
# Run the optimisations under model 1.
KL_pack_1 = KL_pack(nll_pack_q2_1)
opt_res_q2_kl1 = fit_optim(1, KL_pack_1, dataQ2)

# Store the results in P24_optim_1.
P24_optim_1$mle = p = opt_res_q2_kl1$par
P24_optim_1$kl = opt_res_q2_kl1$value
P24_optim_1$gradient = KL_pack_1$gr(p)
P24_optim_1
```

```{r echo=FALSE}
p = opt_res_q2_kl1$par
P24_optim_1
```

```{r P24_bands_1}
# Plot the least-worse mean function under model 1.
P24_bands_1[, 2] = (1 + exp(p)) ^ 2/(1 + exp(p) - x)
P24_true_mean = 3/(2 - x)
plot(dataQ2, pch = 19, 
     main = "Plot of the least-worse mean function under model 1")
lines(x, P24_bands_1[, 2], col = "red")
lines(x, P24_true_mean, col = "blue")
legend("topleft",
  legend = c(TeX(r"($y = \mu(\alpha^{*}, \beta^{*}, x)$)"),
             TeX(r"($y = \mu(\theta_{1}, -\theta_{1}^{2}, x)$)")),
  lty = c(1, 1),
  col = c("blue", "red"))
```

```{r P24_optim_2, warning=FALSE, error=FALSE, eval=FALSE}
# Run the optimisations under model 2.
KL_pack_2 = KL_pack(nll_pack_q2_2)
opt_res_q2_kl2 = fit_optim(2, KL_pack_2, dataQ2)

# Store the results in P24_optim_2.
P24_optim_2$mle = opt_res_q2_kl2$par
P24_optim_2$kl = opt_res_q2_kl2$value
P24_optim_2$gradient = KL_pack_2$gr(opt_res_q2_kl2$par)
P24_optim_2
```

```{r echo=FALSE}
P24_optim_2
```

```{r P24_bands_2}
# Plot the least-worse mean function under model 2.
theta2 = opt_res_q2_kl2$par[1]
tau2 = opt_res_q2_kl2$par[2]
P24_bands_2[, 2] = 1/(theta2 + x * tau2)
plot(dataQ2, pch = 19, 
     main = "Plot of the least-worse mean function under model 2")
lines(x, P24_bands_2[, 2], col = "red")
lines(x, P24_true_mean, col = "blue")
legend("topleft",
  legend = c(TeX(r"($y = \mu(\alpha^{*}, \beta^{*}, x)$)"),
             TeX(r"($y = \mu(\theta_{2}, \tau_{2}, x)$)")),
  lty = c(1, 1),
  col = c("blue", "red"))
```

```{r P24_optim_3, warning=FALSE, error=FALSE, eval=FALSE}
# Run the optimisations under model 3.
KL_pack_3 = KL_pack(nll_pack_q2_3)
opt_res_q2_kl3 = fit_optim(2, KL_pack_3, dataQ2)

# Store the results in P24_optim_3.
P24_optim_3$mle = opt_res_q2_kl3$par
P24_optim_3$kl = opt_res_q2_kl3$value
P24_optim_3$gradient = KL_pack_3$gr(opt_res_q2_kl3$par)
P24_optim_3
```


```{r echo=FALSE}
P24_optim_3
```

```{r P24_bands_3}
# Plot the least-worse mean function under model 3.
p = opt_res_q2_kl3$par[1]
P24_bands_3[, 2] = (1 + exp(p)) ^ 2/(1 + exp(p) - x)
plot(dataQ2, pch = 19, 
     main = "Plot of the least-worse mean function under model 3")
lines(x, P24_bands_3[, 2], col = "red")
lines(x, P24_true_mean, col = "blue")
legend("topleft",
  legend = c(TeX(r"($y = \mu(\alpha^{*}, \beta^{*}, x)$)"),
             TeX(r"($y = \mu(\theta_{3}, -\theta_{3}^{2}, x)$)")),
  lty = c(1, 1),
  col = c("blue", "red"))
```

As $\mathcal{F}_{3}$ has the lowest KL divergence, it is the best 
misspecified model out of the three sub-models.

## Question 2.5

From [Question 2.1], $\mathcal{F}_{\text{full}}$ had the best NIC of
$-1368$. Finding functions $g_{\alpha}$, $g_{\beta}$, and $g_{\phi}$ such that
$g_{\alpha}(\theta_{4}^{*}) = \alpha^{*}$, 
$g_{\beta}(\theta_{4}^{*}) = \beta^{*}$, and 
$g_{\phi}(\theta_{4}^{*}) = \phi^{*}$ would give $\mathcal{F}_{4}$ a better NIC
since the NIC would favour the simpler model $\mathcal{F}_{4}$.
$\mathcal{F}_{2}$ had the next lowest AIC of $-1359$, followed by 
$\mathcal{F}_{3}$ with an NIC of $-1224$. As such, we considered
$g_{\alpha}(\theta_{4}) = \theta_{4}$, 
$g_{\beta}(\theta_{4}) = -\theta_{4}^{a}$, 
$g_{\phi}(\theta_{4}) = \exp(\theta_{4}^{b})$, where $a$ and $b$ are real
constants. By solving for all these conditions, 
it was found that $a \approx 0.75$ and $b \approx 0.5$. 

```{r}
# Estimate the values of a and b.
a = log(-opt_res_q2_full$par[2])/log(opt_res_q2_full$par[1])
b = log(opt_res_q2_full$par[3])/log(opt_res_q2_full$par[1])
print(paste("a: ", a, "; b: ", b))
```

With $g_{\beta}(\theta_{4}) = -\theta_{4}^{3/4}$ and
$g_{\phi}(\theta_{4}) = \exp(\theta_{4}^{1/2})$, $\mathcal{F}_{4}$ achieved
the lowest NIC of $-1372$ out of all the models.

```{r P25_optim, warning=FALSE}
# Define the NLL expression for model 4.
nll_expr_q2_4 = expr(
  lgamma(exp(-theta4 ^ 0.5)) - exp(-theta4 ^ 0.5) * 
    (log(theta4 - x * theta4 ^ 0.75) - 
       theta4 ^ 0.5) + (1 - exp(-theta4 ^ 0.5)) * log(y) +
    exp(-theta4 ^ 0.5) * (theta4 - x * theta4 ^ 0.75) * y
)
nll_pack_q2_4 = deriv_pack(nll_expr_q2_4,
                           c("theta4"),
                           theta_init = 0.5)

# Run the optimisations for model 4.
opt_res_q2_4 = fit_optim(1, nll_pack_q2_4, dataQ2)

# Store the results in P25_optim.
P25_optim$mle = opt_res_q2_4$par
P25_optim$negloglik = opt_res_q2_4$value
P25_optim$hessian = opt_res_q2_4$hessian
P25_optim$gradient = nll_pack_q2_4$gr(opt_res_q2_4$par, dataQ2)
P25_optim$NIC = nic(nll_pack_q2_4, opt_res_q2_4)
P25_optim
```

```{r P25_bands}
# Calculate and plot the mean function and its 95% CI for model 4.
P25_bands = set_P22_bands(P25_bands, 
                         expr(1/(theta4 - x * theta4 ^ 0.75)),
                         c("theta4"), 
                         opt_res_q2_4)
plot_bands(P25_bands, dataQ2, 
           main = "Plot of the mean function and its 95% CI under model 4")
```

# References

Evangelou, E., 2024. MA30084: Generalised Linear Models [Online]. 
University of Bath. Unpublished. Available from:
<https://moodle.bath.ac.uk/mod/resource/view.php?id=1358099> 
[Accessed 5 December 2024].
