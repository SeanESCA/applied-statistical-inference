---
title: 'MA40198: Applied Statistical Inference'
subtitle: "Lab 5: Delta method and Generalised likelihood ratio test"
date: "2024-10-24"
output: html_document
---

[Go back to Moodle page](https://moodle.bath.ac.uk/course/view.php?id=1836&section=11)

This lab is about gaining a solid understanding of the use of asymptotic theory of maximum likelihood to obtain confidence intervals of nonlinear functions as well as  performing hypothesis tests.

# Preliminaries

## Models for count data

The Poisson distribution is the simplest probability model for count data, e.g. taking values $0,1,2,\ldots$. The corresponding probability mass function is given by:

$$
f(y|\mu)=\exp(-\mu)\,\frac{\mu^y}{y!}\quad y=0,1,2,3,4,....
$$

where $\mu=E[Y]=Var[Y]$ which is a special property of the Poisson distribution. A slightly more general probability model is given by  the Negative Binomial distribution with probability mass function:

$$
f(y|\mu,\nu)={\nu+y-1 \choose y }\left(\frac{\nu}{\mu+\nu}\right)^\nu\left(\frac{\mu}{\mu+\nu}\right)^y\quad y=0,1,2,3,4,...
$$

where $\mu=E[Y]>0$ and $\nu>0$. The corresponding variance is given by $$Var[Y]=\mu\left(1+\frac{\mu}{\nu}\right)$$
It is easy to show that when $\nu\to \infty$ then the Negative Binomial distribution tends to the Poisson distribution  with the same mean $\mu$. In this sense, we say that the Negative Binomial  is overdispersed with respect to  the Poisson.

In `R`, the density function of the Negative Binomial  is computed using  `dnbinom(x, size, prob, mu, log = FALSE)`
where 

* `x` is the count  value e.g. $x = 0,1,2,\ldots$.

* `size` corresponds to the parameter $\nu$.

* `mu` corresponds to the mean $\mu$. 

* `prob` is the probability of success $p=\nu/(\mu+\nu)$, similar to the Binomial. Apart from the mean $\mu$, the user should specify either $\nu$ or `prob`.


* `log = TRUE` returns the log density $\log f(x|\mu,\nu)$ and  `log = FALSE` returns the density $f(x|\mu,\nu)$.

For the purposes of obtaining  gradients, it is convenient to use the alternative expression:

$$
{\nu+y-1 \choose y }=\frac{\Gamma(\nu+y)}{\Gamma(y+1)\Gamma(\nu)}
$$

where $\Gamma$ is the usual Gamma function. Relevant functions in R are `gamma` for the Gamma function $\Gamma$ and `lgamma` for the logarithm of the Gamma function. 



## Delta method 

Given an independent sample $y_1|\boldsymbol{x}_1,\ldots,y_n|\boldsymbol{x}_n$ where $\boldsymbol{x}_i\in \rel^p$
where each $\mathcal Y_i|\boldsymbol{x}_i$ follows an unknown density $f_*(y|\boldsymbol{x}_i)$ that is assumed to belong to a parametric family:
$$
\mathcal F = 
\left\{
f(y|\mu(\boldsymbol{\theta}^{(1)},\boldsymbol{x}),\boldsymbol{\theta}^{(2)})
\,:\,
\boldsymbol{\theta}=
\begin{pmatrix}
\boldsymbol{\theta}^{(1)}\\
\boldsymbol{\theta}^{(2)}\\
\end{pmatrix}
\in \boldsymbol{\Theta} \subseteq \rel^{p+m}
\right\}
$$
we have seen in the lecture notes that, under some regularity assumptions, the maximum likelihood estimator $\widehat{\boldsymbol{\theta}}_n(\mathbfcal{Y})$  is asymptotically normal, that is:

$$
\widehat{\boldsymbol{\theta}}_n(\mathbfcal{Y})-\boldsymbol{\theta}^*\stackrel{d}{\rightarrow} N(\boldsymbol{0},\mathbfcal I^{-1}(\boldsymbol{\theta}^*))\quad \mbox{as}\quad n\to \infty
$$

  where $\mathbfcal I({\boldsymbol{\theta}^*})$ is the **Fisher Information matrix** evaluated at $\boldsymbol{\theta}^*$.
  
If we consider a linear function of $\widehat{\boldsymbol{\theta}}_n(\mathbfcal{Y})$, that is
$$
\boldsymbol{g}(\widehat{\boldsymbol{\theta}}_n(\mathbfcal{Y}))=\boldsymbol{A}\widehat{\boldsymbol{\theta}}_n(\mathbfcal{Y})
$$
where $\boldsymbol{A}$ is a constant matrix of dimension $r\times (p+m)$ where $r \leq p+m$ then we clearly have that 

$$
\boldsymbol{A}\widehat{\boldsymbol{\theta}}_n(\mathbfcal{Y})-\boldsymbol{A}\boldsymbol{\theta}^*\stackrel{d}{\rightarrow} N(\boldsymbol{0},\boldsymbol{A}\mathbfcal I^{-1}(\boldsymbol{\theta}^*)\boldsymbol{A}^T)\quad \mbox{as}\quad n\to \infty
$$

Now consider  general vector-valued (and possibly non-linear) smooth function of the form $\boldsymbol{g}:\rel^{p+m} \to \rel^r$ 
that is 
$$
\boldsymbol{g}(\boldsymbol{\theta}) =
\left(
\begin{array}{c}
g_1(\boldsymbol{\theta})\\
g_2(\boldsymbol{\theta})\\
\vdots\\
g_r(\boldsymbol{\theta})\\
\end{array}
\right)
$$

for given differentiable real-valued functions $g_1,\ldots,g_r$ such that $g_i:\rel^{p+m}\to \rel$ for $i=1,\ldots,r$ where $r \leq p+m$.

:::{.callout-note icon=false}

## Delta Method

::: {#prp-delta-method-MLE}

Under the assumptions **A1** to **A5** , a smooth function $\boldsymbol{g}:\rel^{p+m}\to \rel ^r$ of the  maximum likelihood estimator is also asymptotically normal:

$$
\boldsymbol{g}(\widehat{\boldsymbol{\theta}}_n(\mathbfcal{Y}))-\boldsymbol{g}(\boldsymbol{\theta}^*)\stackrel{d}{\rightarrow} N(\boldsymbol{0},\boldsymbol{J}_{\boldsymbol{g}}(\boldsymbol{\theta}^*)\mathbfcal I^{-1}(\boldsymbol{\theta}^*)\boldsymbol{J}^T_{\boldsymbol{g}}(\boldsymbol{\theta}^*))\quad \mbox{as}\quad n\to \infty
$$

where $\boldsymbol{J}_{\boldsymbol{g}}(\boldsymbol{\theta})$ is the Jacobian matrix 
:::
:::


Recall the Jacobian matrix is defined by the following $r \times (p+m)$ derivative matrix:

$$
\boldsymbol{J}_{\boldsymbol{g}}(\boldsymbol{\theta})=
\left(
\begin{array}{c}
\displaystyle \frac{\partial}{\partial \boldsymbol{\theta}^T}g_1(\boldsymbol{\theta})\\
\displaystyle \frac{\partial}{\partial \boldsymbol{\theta}^T}g_2(\boldsymbol{\theta})\\
\vdots\\
\displaystyle \frac{\partial}{\partial \boldsymbol{\theta}^T}g_r(\boldsymbol{\theta})\\
\end{array}
\right)
=
\left(
\begin{array}{cccc}
\displaystyle \frac{\partial g_1(\boldsymbol{\theta})}{\partial \theta_1} & \displaystyle \frac{\partial g_1(\boldsymbol{\theta})}{\partial \theta_2} & \cdots & \displaystyle \frac{\partial g_1(\boldsymbol{\theta})}{\partial \theta_{p+m}}\\
\displaystyle \frac{\partial g_2(\boldsymbol{\theta})}{\partial \theta_1} & \displaystyle \frac{\partial g_2(\boldsymbol{\theta})}{\partial \theta_2} & \cdots & \displaystyle \frac{\partial g_2(\boldsymbol{\theta})}{\partial \theta_{p+m}}\\
\vdots & \vdots & \vdots & \vdots\\
\displaystyle\frac{\partial g_r(\boldsymbol{\theta})}{\partial \theta_1} &  \displaystyle\frac{\partial g_r(\boldsymbol{\theta})}{\partial \theta_2} & \cdots &  \displaystyle\frac{\partial g_r(\boldsymbol{\theta})}{\partial \theta_{p+m}}\\
\end{array}
\right)
$$ 

**Note:** In the same way we have used the  Hessian evaluated at the MLE $\nabla^2_{\! \boldsymbol{\theta}}\phi(\widehat{\boldsymbol{\theta}}|\boldsymbol{y})$ to estimate the Fisher information matrix $\mathbfcal I(\boldsymbol{\theta}^*)$ we can now estimate the asymptotic variance in @prp-delta-method-MLE :
$$
\boldsymbol{J}_{\boldsymbol{g}}(\boldsymbol{\theta}^*)\mathbfcal I^{-1}(\boldsymbol{\theta}^*)\boldsymbol{J}^T_{\boldsymbol{g}}(\boldsymbol{\theta}^*)
$$

with

$$
\boldsymbol{J}_{\boldsymbol{g}}(\widehat{\boldsymbol{\theta}})[\nabla^2_{\! \boldsymbol{\theta}}\phi(\widehat{\boldsymbol{\theta}}|\boldsymbol{y})]^{-1}\boldsymbol{J}^T_{\boldsymbol{g}}(\widehat{\boldsymbol{\theta}})
$$ {#eq-est-asymp-var-delta-MLE}


# Question 1

Use @prp-delta-method-MLE combined with ([-@eq-est-asymp-var-delta-MLE]) to compute confidence bands in the following problems:



1. For the  AIDS epidemic in Belgium data of [Lab 2](https://moodle.bath.ac.uk/pluginfile.php/2648537/mod_resource/content/1/Lab2_new.html) and assuming a Negative Binomial model with mean function $\mu(\boldsymbol{\theta}^*,t)=\exp(\theta^*_1+\theta^*_2\,t)$ where $t$ is the time since 1980, compute and plot the MLE of the probability that there are more than 250 AIDS cases as a function of $t$, that is: 

$$
P_*(Y_t>250|t)=1-\sum_{y=0}^{250} 
{\nu_*+y-1 \choose y }\left(\frac{\nu_*}{\mu(\boldsymbol{\theta}^*,t)+\nu_*}\right)^{\nu_*}\left(\frac{\mu(\boldsymbol{\theta}^*,t)}{\mu(\boldsymbol{\theta}^*,t)+\nu_*}\right)^y\,.
$$ 

Also compute and plot 95\% confidence bands for these probabilities and plot them as a function of $t$ in the range 
$(1,15)$.


```{r}
library(rlang)
mean_expr <- expr(theta1 + theta2 * t)
expr <- expr(-lgamma(v+y)+lgamma(y+1)+lgamma(v) - v*(log(v)-log(exp(!!mean_expr)+v))-y*(!!mean_expr-log(exp(!!mean_expr)+v)))
fn_deriv <- deriv(expr         = expr,
                  namevec      = c("theta1","theta2","v"),
                  function.arg = c("theta1","theta2","v","t","y"),
                  hessian      = T)

nll_fn <- function(theta  = c(1,1),
                   v      = 1,
                   t      = 1,
                   y      = 1){
  
 aux  <- fn_deriv(theta1 = theta[1],
                   theta2 = theta[2],
                   v,
                   t,
                   y)
  
 fn <- sum(as.numeric(aux))
  
 fn
}

nll_grad <- function(theta = c(1,1),
                     v     = 1,
                     t     = 1,
                     y     = 1){
  
  aux  <- fn_deriv(theta1 = theta[1],
                   theta2 = theta[2],
                   v,
                   t,
                   y)
  
  grad <- apply(attr(aux,"gradient"),2,sum)
  
 grad
}

tdat <- 1:13
ydat <- c(12,14,33,50,67,74,123 ,141 ,165 ,204 ,253,246,240 )

optim_1 <-
  optim(par = c(1,1,1),
      fn  = nll_fn,
      gr  = nll_grad,
      method = "BFGS",
      hessian = TRUE,
      t = tdat,
      y = ydat
      )
optim_1

vu <- optim_1$par[3]

```


**KAI Almost correct! You just did not pass the argument v in the correct way! see below for the correct way!**


```{r,warning=FALSE}
nll_fn <- function(theta  = c(1,1,1),
                   t      = 1,
                   y      = 1){
  
 aux  <- fn_deriv(theta1 = theta[1],
                   theta2 = theta[2],
                   v = theta[3],
                   t,
                   y)
  
 fn <- sum(as.numeric(aux))
  
 fn
}

nll_grad <- function(theta = c(1,1,1),
                     t     = 1,
                     y     = 1){
  
  aux  <- fn_deriv(theta1 = theta[1],
                   theta2 = theta[2],
                   v= theta[3],
                   t,
                   y)
  
  grad <- apply(attr(aux,"gradient"),2,sum)
  
 grad
}


optim_1 <-
  optim(par = c(1,1,1),
      fn  = nll_fn,
      gr  = nll_grad,
      method = "BFGS",
      hessian = TRUE,
      t = tdat,
      y = ydat
      )
optim_1

vu <- optim_1$par[3]
```



```{r}

fro <- function(x)
{
mu1 <- exp(optim_1$par[1]+ optim_1$par[2]*x)
 g <- rep(0,250)
 for(i in 0:250)
{
 g[i] <- (gamma(vu+i)/(gamma(i+1)*gamma(vu))) *
    ((vu/(mu1+vu))^vu) * ((mu1/(mu1+vu))^i)
 }
 g <- g[!g %in% c("Inf","NaN")]
 return(1-sum(g))
}
x <- c(1:15)
y <- rep(0,15)
for(i in 1:15)
{
 y[i] <- fro(i)
}

l <- loess(y~x)
plot(x,y)
lines(predict(l), col="red", lwd=2)

```

**KAI You can simply join the lines with `type="l"` rather than using `loess`**

```{r}
plot(x,y,type="l")
```

**KAI** **You need to compoute the asymptotic variances using the delta method! and then compute the confidence bands! see solutions for details!!**


2. For regression data A in [Lab 3](https://moodle.bath.ac.uk/pluginfile.php/2653848/mod_resource/content/5/Lab3_new.html) and assuming a normal model,
compute and plot (as a function of $x$) the MLE of the 90\% quantile of the response given by:
$$y_{90}=\theta_1^*+\theta^*_2\,\,x + z_{0.9}\,\sqrt{\nu_*}=\theta_1^*+\theta_2^*\,\,x + 1.281552\,\sqrt{\nu_*}$$
This is the value that
$$P_*(\mathcal Y\leq y_{90}|X=x)=0.9\,.$$
Here $z_{0.9}$ is the 90\% of a standard normal distribution.
Also compute and plot 95\% confidence bands for these quantiles and plot them as a function of $x$ in the range  $(-3,3)$.

```{r}
data_A <- read.table("https://people.bath.ac.uk/kai21/ASI/data/Lab3_data_A.txt")
mean_expr <- expr(theta1 + theta2 * t)
expr <- expr((0.5)*log(2*pi*vu) + ((y-(!!mean_expr))**2)/(vu*2))
std_var <- expr((y-!!mean_expr)/exp(theta3))
neg_log_density_expr_normal <- 
  expr(
    theta3
    + (1/2)*log(2*pi)
    + ((!!std_var)^2)/2
  )

fn_deriv1 <- deriv(expr         = neg_log_density_expr_normal,
                  namevec      = c("theta1","theta2","theta3"),
                  function.arg = c("theta1","theta2","theta3","t","y"),
                  hessian      = T)

nll_fn <- function(theta  = c(1,1,1),
                   t      = 1,
                   y      = 1){
  
 aux  <- fn_deriv1(theta1 = theta[1],
                   theta2 = theta[2],
                   theta3 = theta[3],
                   t, 
                   y)
  
 fn <- sum(as.numeric(aux))
  
 as.matrix(fn)
}

nll_grad <- function(theta = c(1,1,1),
                     t     = 1,
                     y     = 1){
  
  aux  <- fn_deriv1(theta1 = theta[1],
                   theta2 = theta[2],
                   theta3 = theta[3],
                   t,
                   y)
  
  grad <- apply(attr(aux,"gradient"),2,sum)
  
 as.matrix(grad)
}

optim_2 <-
  optim(par = c(1,1,1),
      fn  = nll_fn,
      gr  = nll_grad,
      method = "BFGS",
      hessian = TRUE,
      t = data_A$x,
      y = data_A$y
      )

optim_2

```


```{r}


curve(optim_2$par[1] + optim_2$par[2]*x + sqrt(exp(optim_2$par[3]))*qnorm(0.9), xlim=c(-3,3), ylab="90% quantile")

```


**KAI : exp(theta3) is already the standard deviation so no need to take sqrt! you can also add the data points for reference see below**

```{r}

curve(optim_2$par[1] + optim_2$par[2]*x + exp(optim_2$par[3])*qnorm(0.9), xlim=c(-3,3), ylab="90% quantile")
points(data_A)


```


```{r}

## function for mean of 90% quantile
from1 <- function(x)
{
 optim_1$par[1] + optim_1$par[2]*x + #sqrt(exp(optim_1$par[3]))*qnorm(0.9) # KAI: changed this
  exp(optim_1$par[3])*qnorm(0.9) 
}
## function for variance of 90% quantile
from <- function(x)
{
  optim_1$hessian[1,1]+(x^2)*optim_1$hessian[2,2]+(qnorm(0.9)^2)*optim_1$hessian[3,3] - 2*x*optim_1$hessian[1,2]-2*(qnorm(0.9))*optim_1$hessian[1,3] - 2*(qnorm(0.9))*x*optim_1$hessian[2,3] 
}
## 95% confidence interval upper bound
curve(from1(x)+sqrt(from(x))*qnorm(0.975), xlim=c(-3,3), ylab="Upper bound for 95% confidence interval")

## 95% confidence interval lower bound
curve(from1(x)-sqrt(from(x))*qnorm(0.975), xlim=c(-3,3), ylab="Lower bound for 95% confidence interval") 
```


**KAI This is incorrect see solutions for details**

# Question 2


Use the generalised likelihood ratio  to test the the following hypotheses: 



1. For the  AIDS epidemic in Belgium data of [Lab 2](https://moodle.bath.ac.uk/pluginfile.php/2648537/mod_resource/content/1/Lab2_new.html) and assuming a Negative Binomial model, test the hypotheses:
$$H_0:\,\theta_2^*=0 \qquad \mbox{vs.}\qquad H_a:\,\theta_2^*\neq 0$$
using an approximate significance level of $\alpha=0.05.$




```{r}
## alternative hypothesis
mean_expr <- expr(theta1 + theta2 * t)
expr <- expr(-lgamma(v+y)+lgamma(y+1)+lgamma(v) - v*(log(v)-log(exp(!!mean_expr)+v))-y*(!!mean_expr-log(exp(!!mean_expr)+v)))
fn_deriv <- deriv(expr         = expr,
                  namevec      = c("theta1","theta2","v"),
                  function.arg = c("theta1","theta2","v","t","y"),
                  hessian      = T)

nll_fn <- function(theta  = c(1,1),
                   v      = 1,
                   t      = 1,
                   y      = 1){
  
 aux  <- fn_deriv(theta1 = theta[1],
                   theta2 = theta[2],
                   v,
                   t,
                   y)
  
 fn <- sum(as.numeric(aux))
  
 fn
}

nll_grad <- function(theta = c(1,1),
                     v     = 1,
                     t     = 1,
                     y     = 1){
  
  aux  <- fn_deriv(theta1 = theta[1],
                   theta2 = theta[2],
                   v,
                   t,
                   y)
  
  grad <- apply(attr(aux,"gradient"),2,sum)
  
 grad
}


optim_3 <-
  optim(par = c(1,1, 1),
      fn  = nll_fn,
      gr  = nll_grad,
      method = "BFGS",
      hessian = TRUE,
      t = tdat,
      y = ydat
      )
optim_3


```

**KAI same error in passing 'v' argument. Code below has already been corrected! You do not need the above code as it was done in Q1!!! code below has already been corrected*

```{r}

## null hypothesis
mean_expr <- expr(theta1)
expr <- expr(-lgamma(v+y)+lgamma(y+1)+lgamma(v) - v*(log(v)-log(exp(!!mean_expr)+v))-y*(!!mean_expr-log(exp(!!mean_expr)+v)))
fn_deriv <- deriv(expr         = expr,
                  namevec      = c("theta1","v"),
                  function.arg = c("theta1","v","t","y"),
                  hessian      = T)

nll_fn <- function(theta  = c(1,1),
                   t      = 1,
                   y      = 1){
  
 aux  <- fn_deriv(theta1 = theta[1],
                   v =theta[2],
                   t,
                   y)
  
 fn <- sum(as.numeric(aux))
  
 fn
}

nll_grad <- function(theta = c(1,1),
                     t     = 1,
                     y     = 1){
  
  aux  <- fn_deriv(theta1 = theta[1],
                   v =theta[2],
                   t,
                   y)
  
  grad <- apply(attr(aux,"gradient"),2,sum)
  
 grad
}

optim_4 <-
  optim(par = c(2,2),
      fn  = nll_fn,
      gr  = nll_grad,
      method = "BFGS",
      hessian = TRUE,
      t = tdat,
      y = ydat
      )
optim_4

```

```{r}

2*(optim_4$value/optim_3$value)
# KAI: Incorrect! is the difference not the ratio as it is in the log scale!

2*(optim_4$value-optim_1$value)

qchisq(0.95, df=2) ## df = 3 parameters - 1 restriction
## Not enough to reject null

```


2. For regression data A in [Lab 3](https://moodle.bath.ac.uk/pluginfile.php/2653848/mod_resource/content/5/Lab3_new.html)  and assuming a normal model, test the hypotheses:
$$H_0:\,\theta_2^*=0 \qquad \mbox{vs.}\qquad H_a:\,\theta_2^*\neq 0$$
using an approximate significance level of $\alpha=0.05.$

```{r}
##Null hypothesis
mean_expr <- expr(theta1 + theta2 * t)
ell_normal_expr <- expr(0.5 * log(2 * pi) + theta3 + ((y - !!mean_expr)**2) / exp(2 * theta3))
fn_deriv <- deriv(expr         = ell_normal_expr,
                  namevec      = c("theta1","theta2","theta3"),
                  function.arg = c("theta1","theta2","theta3","t","y"),
                  hessian      = T)

nll_fn <- function(theta  = c(1,1,1),
                   t      = 1,
                   y      = 1){
  
 aux  <- fn_deriv(theta1 = theta[1],
                  theta2 = theta[2],
                  theta3 = theta[3],
                   t,
                   y)
  
 fn <- sum(as.numeric(aux))
  
 fn
}

nll_grad <- function(theta = c(1,1,1),
                     t     = 1,
                     y     = 1){
  
  aux  <- fn_deriv(theta1 = theta[1],
                   theta2 = theta[2],
                   theta3 = theta[3],
                   t,
                   y)
  
  grad <- apply(attr(aux,"gradient"),2,sum)
  
 grad
}

optim_5 <-
  optim(par = c(1,1,1),
      fn  = nll_fn,
      gr  = nll_grad,
      method = "BFGS",
      hessian = TRUE,
      t = data_A$x,
      y = data_A$y
      )

optim_5

```


```{r}
## Alternative hypothesis
mean_expr <- expr(theta1)
ell_normal_expr <- expr(0.5 * log(2 * pi) + theta3 + ((y - !!mean_expr)**2) / exp(2 * theta3))

fn_deriv <- deriv(expr         = ell_normal_expr,
                  namevec      = c("theta1","theta3"),
                  function.arg = c("theta1","theta3","t","y"),
                  hessian      = T)

nll_fn <- function(theta  = c(1,1),
                   t      = 1,
                   y      = 1){
  
 aux  <- fn_deriv(theta1 = theta[1],
                  theta3 = theta[2],
                   t,
                   y)
  
 fn <- sum(as.numeric(aux))
  
 fn
}

nll_grad <- function(theta = c(1,1),
                     t     = 1,
                     y     = 1){
  
  aux  <- fn_deriv(theta1 = theta[1],
                   theta3 = theta[2],
                   t,
                   y)
  
  grad <- apply(attr(aux,"gradient"),2,sum)
  
 grad
}

optim_6 <-
  optim(par = c(1,1),
      fn  = nll_fn,
      gr  = nll_grad,
      method = "BFGS",
      hessian = TRUE,
      t = data_A$x,
      y = data_A$y
      )
optim_6

2*(optim_5$value/optim_6$value)

# KAI: same erro as before, correct answer below

2*(optim_6$value-optim_5$value)

qchisq(p=0.95, df=2)
## Accept null
```


```{r}

```

