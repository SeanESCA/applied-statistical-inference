---
title: 'MA40198: Applied Statistical Inference'
subtitle: "Lab 6: Asymptotic simulations"
author: "Group 13: Sean Soon and Shaveen Colambage"
output: html_document
---

[Go back to Moodle page](https://moodle.bath.ac.uk/course/view.php?id=1836&section=11)

This lab is about using simulations to understand the asymptotic results shown in the lecture notes. 



## Question 1

Generate $N=10,000$ independent samples $\boldsymbol{y}_1,\ldots, \boldsymbol{y}_N$ where  each $\boldsymbol{y}_i$ is a random sample of size
$2n$ from a Negative Binomial distribution with  population size $\nu^*=1/2$ and mean $\mu^*_1=5$ for the first $n$ observations of $\boldsymbol{y}_i$ and mean $\mu^*_2=30$ for the second $n$ observations of $\boldsymbol{y}_i$. You may use the `R` function `rnbinom` for this purpose. 

Save all the samples in the rows of a matrix of dimension $N\times (2n)$. Call this matrix `y_samples`

## Solution to Question 1 {-}


## Question 2

Consider the parametric model where the first $n$ observations are Negative Binomial with mean $\exp(\theta^*_1)$ and size $\exp(\theta^*_3)$ and the following $n$ observations have mean $\exp(\theta^*_1+\theta^*_2)$ and  size $\exp(\theta^*_3)$ for some unknown $\bm{\theta}^*=(\theta^*_1,\theta^*_2,\theta^*_3)^T$. 



For each of the $N=10,000$ samples (that is, each row of `y_samples`) compute the standardised vector: 

$$
\widehat{\bm{\theta}}_{std}(\bm{y}):=[\nabla^2_{\!\boldsymbol{\theta}}\phi(\widehat{\boldsymbol{\theta}}|\boldsymbol{y})]^{1/2}
(\widehat{\boldsymbol{\theta}}-\boldsymbol{\theta}^*)
$$



So at the end we get a sample: 

$$
\widehat{\boldsymbol{\theta}}_{std}(\boldsymbol{y}_1),\ldots\widehat{\boldsymbol{\theta}}_{std}(\boldsymbol{y}_N)
$$ 

from the  distribution of the standardised MLE (not from the asymptotic normal distribution given in [Proposition 3.2 of the lecture notes](https://moodle.bath.ac.uk/pluginfile.php/2633371/mod_resource/content/9/docs/02-likelihood.html#estimators-of-the-asymptotic-variance).



Save the sample of stardardised MLE vectors in the rows of a matrix of dimension $N\times 3$. Call this matrix `mles_std`


Plot a histogram of each entry of the stardardised MLEs and compare it with the pdf of a standard normal distribution.

Plot a scatter plot of each of the three possible pairs of the stardardised MLE  and compare it with the contours of a standard  bivariate normal distribution.

What can you say about the normal approximation to the distribution of the the stardardised  MLE? 



## Solution to Question 2 {-}



## Question 3

Generate $N=10,000$ independent samples $\boldsymbol{y}_1,\ldots, \boldsymbol{y}_N$ where  each $\boldsymbol{y}_i$ is a random sample of size
$2n$ from a Negative Binomial distribution with  population size $\nu^*=1/2$ and mean $\mu^*_1=5$ for the first $n$ observations of $\boldsymbol{y}_i$ and mean $\mu^*_2=5$ for the second $n$ observations of $\boldsymbol{y}_i$. 

Save all the samples in the rows of a matrix of dimension $N\times (2n)$. Call this matrix `y_samples_H0`

Consider the parametric model where the first $n$ observations are Negative Binomial with mean $\exp(\theta^*_1)$ and size $\exp(\theta^*_3)$ and the following $n$ observations have mean $\exp(\theta^*_1+\theta^*_2)$ and  size $\exp(\theta^*_3)$ for some unknown $\bm{\theta}^*=(\theta^*_1,\theta^*_2,\theta^*_3)^T$. For each of the $N=10,000$ samples (that is, each row of `y_samples_H0`) compute the minimised value of the corresponding negative loglikelihood $\phi(\widehat{\bm{\theta}}|\bm{y})$.

Also consider the null model where the $2n$ observations are Negative Binomial with mean $\exp(\theta^*_1)$ and size $\exp(\theta^*_2)$ for some unknown $\bm{\theta}^*=(\theta^*_1,\theta^*_2)^T$. For each of the $N=10,000$ samples (that is, each row of `y_samples_H0`) compute the minimised value of the corresponding negative loglikelihood $\phi_0(\widehat{\bm{\theta}}_0|\bm{y})$.

Compute the generalised loglikelihood ratio statistic

$$T(\bm{y})=2[\phi_0(\widehat{\bm{\theta}}_0|\bm{y})-\phi(\widehat{\bm{\theta}}|\bm{y})]$$





So at the end we get a sample 

$$
T(\bm{y}_1),\ldots,T(\bm{y}_N)
$$ 

from the  distribution of the generalised loglikelihood ratio statistic



Save the samples of the generalised loglikelihood ratio statistic in a vector of length $N$. Call this vector `glrts`

Compare the histogram of the samples on `glrts` with the pdf of  a chi-squred distributions with one degree of freedom.

## Solution to Question 3 {-}



## Question 4


Using the observations in `y_samples` generated in Q1,
consider the (incorrect) model where the first $n$ observations are Poisson with mean $\exp(\theta^*_1)$ and the following $n$ observations are Poisson with mean $\exp(\theta^*_1+\theta^*_2)$ for some unknown $\bm{\theta}^*=(\theta^*_1,\theta^*_2)^T$. 


Compute the least worse value $\bm{\theta}^\dagger\in \rel^2$ that minimises the KL divergence

$$
\sum_{i=1}^{2n}KL(f(y_i|\bm{\theta},\bm{x}_i),f_*(y_i|\bm{x}_i))
$$

where $f(y_i|\bm{\theta},\bm{x}_i)$ is the Poisson density for the $i$-th observation and $f_*(y_i|\bm{x}_i)$ is the Negative Binomial density for the $i$-th observation at the true value.

For each of the $N=10,000$ samples (that is, each row of `y_samples`) compute the standardised vector: 


$$
\widehat{\boldsymbol{\theta}}_{std}(\boldsymbol{y}):=
\left\{
\left[\wh{\mathbfcal J (\bm{\theta}^\dagger})\right]^{-1}
\wh{\mathbfcal K (\bm{\theta}^\dagger)}
\left[\wh{\mathbfcal J (\bm{\theta}^\dagger})\right]^{-1}
\right\}^{-1/2}
(\wh{\bm{\theta}}-\bm{\theta}^\dagger)
$$

where

$$
\wh{\mathbfcal K (\bm{\theta}^\dagger)}=\sum_{i=1}^n\left[\nabla_{\!\bm{\theta}}\, \log f(y_i|\wh{\bm{\theta}},\bm{x}_i)\right]\left[\nabla_{\!\bm{\theta}}\, \log f(y_i|\wh{\bm{\theta}},\bm{x}_i)\right]^T\,,
\qquad
\wh{\mathbfcal J (\bm{\theta}^\dagger})=
\nabla^2_{\!\bm{\theta}}\, \phi(\wh{\bm{\theta}}|\bm{y})
$$

and

$$
\phi(\bm{\theta}|\bm{y})=-\sum_{i=1}^n \log f(y_i|\bm{\theta},\bm{x}_i)
$$

So at the end we get a sample 

$$
\widehat{\boldsymbol{\theta}}_{std}(\boldsymbol{y}_1),\ldots\widehat{\boldsymbol{\theta}}_{std}(\boldsymbol{y}_N)
$$ 

from the  distribution of the standardised MLE (not from the asymptotic normal distribution given in [Proposition 3.7 of the lecture notes](https://moodle.bath.ac.uk/pluginfile.php/2633371/mod_resource/content/9/docs/02-likelihood.html#incorrectly-specified-models).



Save the sample of stardardised MLE vectors in the rows of a matrix of dimension $N\times 2$. Call this matrix `mles_std_wrong`


Plot a histogram of each entry of the stardardised MLEs and compare it with the pdf of a standard normal distribution.

Plot a scatter plot of the pair of stardardised MLEs  and compare it with the contours of a standard  standard normal distribution.

What can you say about the normal approximation to the distribution of the stardardised  MLE? 



## Solution to Question 4 {-}


# Optimisation guidelines {#optim-guidelines}

* You should use at least 100 different random starting points for any optimisation performed. This is to make sure you obtain the smallest value possible (for minimisation) of the objective function. To generate random initial points you can use normal random numbers.

* For each optimisation, you should use the BFGS algorithm and you should provide the gradient function to the algorithm. You can use the R function `optim`. You may use either automatic differentiation via `deriv` or numerical differentiation via the library `numDeriv`.

* You should use appropriate reparametrisations (usually involving $\log$) to perform unconstrained optimisation, that is, the parameters to be optimised over, should not be constrained to lie in a bounded set. 
 
 